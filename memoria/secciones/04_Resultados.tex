\chapter{Results}

In this chapter we will show the results obtained in this work. They will be divided between the three metrics we chose to monitor (see section Experiment Design in the Development chapter).

\section{Rasterization Baseline}
First of all and as a sanity check, we will compare the performance of a renderer using ray tracing to one drawing the same geometry through rasterization. The model chosen for this will be the Viking Room from the Vulkan Tutorial \cite{VulkanTutorial}.

In the first place we will look at how video memory usage changes as the frame sizes get bigger. We can see a comparison of how much memory both renderers employ in graph \ref{memory-usage-comparison-graph}. We see an expected increase in memory consumption as the frame buffer resolutions get bigger, as well as a huge difference, of about an order of magnitude, between the two methods for any given resolution. This is to be expected not only because a bigger frame buffer will require more memory to be stored, but also because the ray tracer requires to store an acceleration structure as well as all the possible shaders bound at the same time.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1.0\textwidth]{figuras/vulkan-memory-usage-comparison.png}
    \caption{GPU memory consumption when rendering the Viking Room 3D model at a range of reasonable resolutions, using ray traced and traditionally rasterized graphics. We can see a significant difference between the two, as well as an expected increase as resolutions get higher.}
    \label{memory-usage-comparison-graph}
\end{figure}

Rasterized renderers have an initialization process quite different than raytraced ones since they do not make use of an acceleration structure. Therefore, we cannot compare it's building time in this context.

To finish this part, we will compare frame times from both renderers in the same fashion as the memory usage one: with the same model (Viking Room), we'll run them for 1000 frames and take the average time it took to render them at different screen resolutions. We can see how they stack up in figure \ref{frametimes-comparison-graph}. This is very similar to the memory usage comparison in that both frame times increase as the frame buffer resolution grows, with the rasterized renderer finishing the job much quicker than it's counterpart. In any case, in this instance the performance difference is even more exaggerated between the two, with rasterized graphics greatly surpassing ray traced ones.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1.0\textwidth]{figuras/vulkan-frametimes-comparison.png}
    \caption{Single frame rendering time when drawing the Viking Room 3D model at a range of reasonable resolution, using ray traced and traditionally rasterized graphics. As with the memory consumption graph, we see how the two of them take longer to finish in higher resolutions, while the rasterized renderer greatly outperforms the ray traced one.}
    \label{frametimes-comparison-graph}
\end{figure}


\clearpage
\section{Memory Usage}
To start the real comparison between Vulkan and OptiX we will look at the memory usage in GPU across both libraries, all models and all resolutions. We can see how much each model consumes in graph \ref{memory-usage-raytraced-comparison-graph}.

This is the first example of a trend we will start seeing throughout this chapter. Vulkan, either despite or thanks to it's explicitness when developing applications, is able to load and render the same ammount of geometry and detail as OptiX while requiring only a fraction of the resources, in this case video memory. This remains true while increasing the frame buffer size, with both libraries increasing the required memory at a similar rate. One last remarkable thing is the Hairball test case. Here we see the memory consumption for both libraries is very similar, which suggests the two technologies tend to use a similar ammount of resources when triangle counts tend to infinity.

\begin{figure}
    \subfloat[BMW]{\includegraphics[width = 3in]{figuras/bmw-memory-usage-comparison.png}}
    \subfloat[Sponza]{\includegraphics[width = 3in]{figuras/sponza-memory-usage-comparison.png}}\\
    \centering
    \subfloat[Hairball]{\includegraphics[width = 3in]{figuras/hairball-memory-usage-comparison.png}}
    \caption{Memory consumption of OptiX and Vulkan (raytraced) when rendering the 3D models BMW, Sponza and Hairball.}
    \label{memory-usage-raytraced-comparison-graph}
\end{figure}

\clearpage
\section{Acceleration Structure Build Time}
Next up we will be looking at the Acceleration Structure Build time across both libraries. Although in our experiments this was only done once per program execution, applications that perform animations or other form of dynamic geometry may need to partly rebuild their AS as frequent as once per frame. Thus, the time it takes to do so (although not all of it) could be counting towards the total time it takes to process a full application cycle (along with rendering, and simulation, etc.).

If we plot the Acceleration Structure Building Times in the same way as we did with the memory consumption, we get the graphs at figure \ref{as-build-time-comparison-graph}.

\begin{figure}
    \subfloat[BMW]{\includegraphics[width = 3in]{figuras/bmw-acceleration-structure-build-time-comparison.png}}
    \subfloat[Sponza]{\includegraphics[width = 3in]{figuras/sponza-acceleration-structure-build-time-comparison.png}}\\
    \centering
    \subfloat[Hairball]{\includegraphics[width = 3in]{figuras/hairball-acceleration-structure-build-time-comparison.png}}
    \caption{Acceleration Structure Build Time of OptiX and Vulkan (raytraced) when rendering the 3D models BMW, Sponza and Hairball. Sorted by rendering resolution.}
    \label{as-build-time-comparison-graph}
\end{figure}

We can plainly see how the rendering resolution does not affect the accel build time. This is to be expected, since a higher pixel count works with the same Acceleration Structure as a lower pixel count. We will now rearrange the plots in figure \ref{as-build-time-geometry-comparison-graph} to see how geometry ammount affects acceleration structure build times.

A curious phenomenon is shown here. We initially expected the Hairball model to have the greatest acceleration structure build time. This is true for Vulkan where the triangle count is directly proportional to the build time. In OptiX however, the first spot is granted to Sponza, with about 10 times less triangles than the Hairball. This could be due to this model having a wider variety of shapes formed by it's triangles, which could be harder to optimize, as opposed to the hairball, where everything is more similar and packed together.

When comparing both libraries, even in the most discrepant case (Hairball), we see Vulkan being at least slightly faster, around a 20\% on average across all cases and in most of them simply a lot faster than OptiX when building the AS.

\begin{figure}
    \subfloat[1280x720]{\includegraphics[width = 3in]{figuras/1280x720-acceleration-structure-geometry-build-time-comparison.png}}
    \subfloat[1920x1080]{\includegraphics[width = 3in]{figuras/1920x1080-acceleration-structure-geometry-build-time-comparison.png}}\\
    \subfloat[2560x1440]{\includegraphics[width = 3in]{figuras/2560x1440-acceleration-structure-geometry-build-time-comparison.png}}
    \subfloat[3840x2160]{\includegraphics[width = 3in]{figuras/3840x2160-acceleration-structure-geometry-build-time-comparison.png}}
    \caption{Acceleration Structure Build Time of OptiX and Vulkan (raytraced) when rendering the 3D models BMW, Sponza and Hairball. Sorted by model.}
    \label{as-build-time-geometry-comparison-graph}
\end{figure}

To further investigate why Sponza takes that much longer to build than other models, we first timed each part of the building process separately. We see this in figure \ref{as-build-time-decomposed}. We observe how the distribution of times for each model is completely different. The Hairball model takes the longest by far to build it's TLAS, while Sponza takes it's much longer time mostly building the Bottom Level part, taking as little as the BMW one in the Top Level stage.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1.0\textwidth]{figuras/optix-accelbuildtimes-decomposed.png}
    \caption{Acceleration Structure Build Times for each model, divided between the TLAS and BLAS phases. We see great discrepancies in the distribution of time in each case.}
    \label{as-build-time-decomposed}
\end{figure}

Seeing this, we decided to expand this metric's models testing pool to Human, ISCV2 and Gallery. In figure \ref{as-extra-build-time-decomposed} we see how their build times stack up to one another. In sight of these new data points we appreciate that a longer build time seems to come not only from the triangle count, but rather from the presence of textures, that seem to extend the Top Level Acceleration Structure build time.


\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1.0\textwidth]{figuras/optix-extra-accelbuildtimes-decomposed.png}
    \caption{Acceleration Structure Build Times divided between the TLAS and BLAS phases for the extra set of models (Human, ISCV2 and Gallery). We see how the presence of textures makes for a longer TLAS build time.}
    \label{as-extra-build-time-decomposed}
\end{figure}


\clearpage
\section{Frame Time}
Finally we will look at how long each technology takes to render a full frame. This could be considered the most important metric to monitor, since rendering a frame is something that will happen as often as possible, and therefore the ammount of time it takes to do so will be more prevalent than updating or initializing geometry structures.

As mentioned before, each frame will be rendered to a frame buffer and we will only count the time it takes to do exactly that, although we will then display them in a window so we can see what the renderer is doing.

In a modern operating system, the time it takes to perform a certain task can vary a lot depending on what other processes are running at the time. To counteract this, we timed the render of 1000 frames for each parameter combination and averaged them. An example of that can be seen in figure \ref{frametimes-outlier-graph}.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1.0\textwidth]{figuras/frametime-outlier.png}
    \caption{Time taken for rendering 1000 frames when drawing the BMW 3D model at 1280x720, using ray tracing. We see how the first frame takes much longer than the rest of them to draw.}
    \label{frametimes-outlier-graph}
\end{figure}

We decided to remove the first frame time from this average since, as you can see, it tends to be a statistical outlier, messing with "real world" appreciable results. The same graph from \ref{frametimes-outlier} results thus in \ref{frametimes-no-outlier}.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1.0\textwidth]{figuras/frametime-no-outlier.png}
    \caption{Time taken for rendering 1000 frames when drawing the BMW 3D model at 1280x720, using ray tracing. We have removed the first frame time since it took too long to draw in comparison with the rest.}
    \label{frametimes-no-outlier-graph}
\end{figure}

With this preprocessing in place, we will first look at an overview for all models and resolutions in figure \ref{frametimes-overview-graph}. For the most part the graphs show what one might expect: render times get larger as screen resolution increases, with OptiX taking the longest across all cases. There is however a slightly odd yet expected behaviour. As we see, the model taking the longer to draw in both libraries is Sponza. The much bigger Hairball shows times more similar to the BMW model, with about 10 times more geometry. This suggests that complexity in shapes and textures may be more costly to render than a higher triangle count. 

This discrepancy between triangle count and processing time reminds to the Acceleration Structure Build Times, where OptiX also struggled the most to optimize the Sponza geometry compared to the others. The main difference is here both libraries have the most trouble to draw this particular scene.

\begin{figure}
    \subfloat[BMW]{\includegraphics[width = 3in]{figuras/bmw-frametimes-comparison.png}}
    \subfloat[Sponza]{\includegraphics[width = 3in]{figuras/sponza-frametimes-comparison.png}}\\
    \centering
    \subfloat[Hairball]{\includegraphics[width = 3in]{figuras/hairball-frametimes-comparison.png}}
    \caption{Frame rendering times for each 3D model (BMW, Hairball and Sponza) across both libraries and 4 reasonable resolutions, from 720p to 4k.}
    \label{frametimes-overview-graph}
\end{figure}


\clearpage
\section{Hardware comparison}
As mentioned in the Development chapter, we ran our software in all the machines we could gather from friends, family and fellow students. This will help us gauge how the same workload is handled by different GPUs, how CPUs affect performance while maintaining the same GPU and even how both libraries behave inside a virtual machine.

\subsection{Same GPU, changing CPU}
We'll start by comparing how a difference in CPU power affects all the metrics we're monitoring, except for memory usage of course. We have a couple of pairs of systems with the same GPUs and different processors. We will be looking at two systems. One has an AMD Ryzen 5 3700 BOX and the other an Intel Core i9-12900F. Both of them are using an Nvidia 2070 Super. %On the one hand we have two \textit{2070 Super}'s with an AMD Ryzen 5 3700 BOX and 7 3800x respectively, and on the other we have two \textit{2600}'s, one with an Intel Core i9-12900F and the other with a % TODO 

We will start with OptiX. In graph \ref{optix-2070-super-comparison} we see how acceleration structure build time and frame times compare to one another when using two machines with a \textit{2070 Super} and AMD Ryzen 5 and 7 processors respectively. Some of the results data went missing when trying to get the software to run on other people's systems, so we will not be comparing all the models' metrics every time. Additionally, we have only compared the render time averages at a 1920x1080 resolution to simplify the graphs.

In both cases we see a marginal increase in performance (less time consumption) in the system with the more powerful CPU. In all cases it's small enough that it could be considered a fluke, being a difference smaller than a 10\% every time. However, since it happens accross all tested models and all the averages in render time, we can determine it does have an impact on render and acceleration structure build times.

\begin{figure}
    \subfloat[Acceleration Structure Build Time]{\includegraphics[width = 3in]{figuras/accelbuildtime-2070super-comparison.png}}
    \subfloat[Frame Times]{\includegraphics[width = 3in]{figuras/frametimes-2070super-comparison.png}}\\
    \caption{Acceleration Structure Build Time and Frame Render Time comparison in OptiX when using an Nvidia 2070 Super in two systems with different CPUs. We see how a faster CPU means better performance in both cases.}
    \label{optix-2070-super-comparison}
\end{figure}

In figure \ref{vulkan-2070-super-comparison} we see the same comparison for Vulkan, only this time using a larger pool of models. Note how in this case the model of CPU doesn't seem as important, with both of them registering shorter times depending on the metric and scene. We assume from this that Vulkan is not as dependent on the CPU as OptiX when it comes to building an acceleration structure or rendering.

\begin{figure}
    \subfloat[Acceleration Structure Build Time]{\includegraphics[width = 3in]{figuras/vulkan-accelbuildtime-2070super-comparison.png}}
    \subfloat[Frame Times]{\includegraphics[width = 3in]{figuras/vulkan-frametimes-2070super-comparison.png}}\\
    \caption{Acceleration Structure Build Time and Frame Render Time comparison in Vulkan when using an Nvidia 2070 Super in two systems with different CPUs. We see more variation in which system does better, in contrast with the previous OptiX test.}
    \label{vulkan-2070-super-comparison}
\end{figure}


\clearpage
\subsection{Virtualization effects}
Virtual machines are a staple of computing, indispensable when dispatching a job to a server or running experiments in isolation. During the development of this work we came across a curious use of these systems, in which people who primarily use Linux run a small virtual machine for specific purposes, such as gaming or having access to some software. 

VM's dedicated to gaming or any other intensively graphically intensive work use GPU passthrough to have access to near bare metal performance. We will now analyze to what extent this is true, by running our benchmark in a VM dedicated almost exclusively to gaming by it's user. According to them, it's performance is almost identical to running the games bare metal, with some minimal additional stuttering. As shown in the Development chapter, this machine uses 12 out of the 16 cores available in it's processor.

The comparison will not be exact, since the most similar system we had access to had an Intel Core i7, compared to the i9 running the virtual machine. However, the GPU is the same across both systems, which should be close enough for our purposes. Unfortunately, due to driver issues, we will only be looking at the Vulkan data for this case, since we couldn't get the OptiX ray tracer to run on the Virtual Machine.

We'll start by looking at the Acceleration Structure build time in \ref{vulkan-accelbuildtime-bm-vm-comparison}. We see how the virtualized system is three times slower than the bare metal one for this task, despite having a more powerful processor. This makes sense to an extent, since this task is relatively CPU-intensive, even though we just saw Vulkan depends less on this component than on the graphics card.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1.0\textwidth]{figuras/vulkan-accelbuildtime-baremetal-virtualized-comparison.png}
    \caption{Acceleration Structure Build Times for Vulkan in a Virtual Machine and running bare metal. We see how the bare metal solution manages to get a significantly lower time than the virtualized one.}
    \label{vulkan-accelbuildtime-bm-vm-comparison}
\end{figure}

Next up we will look at frame render times across multiple models in figure \ref{vulkan-frame-times-bm-vm-comparison}. We see how, opposite to the acceleration structure build time, the virtual machine manages to draw frames marginally faster than the bare metal environment. The difference between the two is so small that we could attribute it to a fluke, concluding that the CPU isn't really relevant when rendering a static scene and that the render time will in essence only depend on the GPU's capabilities.

\begin{figure}[hbt!]
    \centering
    \includegraphics[width=1.0\textwidth]{figuras/vulkan-frametimes-baremetal-virtualized-comparison.png}
    \caption{Frame render times in Vulkan across two machines with the same GPU (Nvidia 3080). One of them is running the renderer under a virtual machine and the other is doing so bare metal. We see a slight advantage towards the virtualized approach in this side, which could be due to a mere coincidence. This would make the GPU the only important element when drawing a given scene.}
    \label{vulkan-frame-times-bm-vm-comparison}
\end{figure}
